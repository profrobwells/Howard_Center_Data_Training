---
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

::: {style="text-align: center; color: #337DFF;"}
# How Journalists Ask Questions of Data

### Digital Certificate Program

#### Philip Merrill College of Journalism

(ver May 19, 2025)
:::

<br>

::: {style="text-align: center;"}
<img src="assets/Merrill_logo.png" width="370" height="200"/>
:::

::: {style="font-weight:bold;"}
``` markdown
Rob Wells, Ph.D., Associate Professor
```
:::

#Lesson 1: Looking for stories in data

[Slides for this
talk](https://docs.google.com/presentation/d/1WsNQBQ44nS1CeSWzmaQaf3BHP_iTm8domTRVlBIQWuw/edit?usp=sharing)

In this lesson, we will cover the following topics:

-   How data analysis empowers journalists and focuses their research

-   How to apply standard news values when working with data

-   How to find stories in outliers; stories measuring deviations from
    average behavior.

Learning Outcome: You will understand how journalists use data to find
newsworthy material.

As journalists, we seek the unusual, the novel and the unexpected when
hunting for stories. Deirdre O'Neill and Tony Harcup described the
haziness of how we determine what is newsworthy. A common feature
involves information that is new, relevant to readers, topical, and
often out of the ordinary. The tools in this exercise will help us find
such patterns.

Data analysis can unearth previously undisclosed patterns. Data analysis
can offer context - is something the biggest, smallest, about average?
Data analysis can lead to discoveries even the government didn’t
realize.

For example, let's look at a [typical city
budget.](https://docs.google.com/spreadsheets/d/1cchcJljm7uJsFy0TPbmCrb6aeAxx3dbY/edit?usp=sharing&ouid=102324743793755798467&rtpof=true&sd=true)
Examine the categories for the city council, police, fire, parks and so
forth. We see total spending is up and the same with revenues. Now apply
a basic tool, the percentage change calculation, to see how spending by
different departments. The big winners are the general fund, up 20%, and
police, up 14%, whereas the biggest losers are the parks, down 12% and
city council, down 9%. Then note the average increase, which was 6%.

Armed with this information, we have a winners and losers story: parks
and public works emerge as losers whereas the cops and the city's
general fund are winners. Trees and playgrounds lose whereas guns and
jails win. These simple calculations allow journalists to make clear,
declarative statements about a list of numbers and humanize the findings
in understandable ways. They also provide insight about the government
operations that officials either didn't realize or didn't want to share.

Yet getting the numbers is not enough. The power of data analysis is it
helps journalists better focus their efforts and find good people to
interview. The data is a great first step but the best stories come
after journalists do the necessary reporting to offer the human context.

We are working with global life expectancy data from
[gapminder.org](https://www.gapminder.org/data/), which covers 196
countries. This data spans from the year 1800 and has projections
extending to 2100.

Your editor gives you the Gapminder dataset and asks for a story in an
hour. Where to start? First, we want to know who is winning: which
country has the highest life expectancy? We won't reveal the answer here
because you're going to use code to find it.

Next, we need to know the countries that are the worst off in terms of
life expectancy. Again, the code will help us find these countries. This
leads us to report an easy story: “People living in Country_XYZ live the
61% as long as Country_ABC, according to new data.”

In essence, we are looking for outliers, and these tools help describe
such patterns.

If your country isn't on the polar ends of the spectrum, we need to
learn how it compares to others. That's where averages and medians come
into play. Data journalists like to understand the typical case in a
dataset, and so we will calculate the mean or median value.

To kick that up a notch, we will look at where your country falls in the
distribution. Is it grouped in the lowest quarter of the countries or in
the highest tier? We can measure distribution by quartile.

Quiz for Lesson #1

Q: Select the following true statements about using data analysis to
find stories.

a: TRUE: You can identify outliers, the winners and losers, and provide
details

b: FALSE: The best data stories don't require additional reporting

c: FALSE: Journalists should not perform their own calculations on
government or business documents without first checking with the
sources.

d: TRUE: Powerful and interesting stories can be told based on simple
percentage change calculations.

Q: Your editor assigned you to write a story about the new city budget.
It shows overall spending increased \$1.3 million, library funding was
up \$90,000 and police funding was up \$400,000. What would a data
journalist do next?

a) FALSE: Produce a story with just the numbers

b) TRUE: Perform percentage change calculations and determine the
averages for context on the changes

c) TRUE: Conduct interviews with police officials, librarians, city
council members and members of the public to contextualize the changes

d) FALSE: Write a lead focusing on the city's press release that
emphasized positive news about police funding

## What is R?

The basic R programming language comes equipped with the bread and
butter math and visualization functions. You can perform basic
mathematical operatons, generate data visualizations, produce
probability distributions, filter and transform data using the "base R"
commands.

What makes R so dynamic is the thousands of external libraries (often
called “packages”) that allow you to build websites, publish books,
generate interactive graphics and produce topic models.

We will be using the Swiss Army knife of R packages, tidyverse.
Tidyverse comes with nine basic packages loaded with these strange nerd
names such as dplyr and readr and ggplot2. We'll use parts of this
package in each lesson.

By loading this one package, you can:

-   Load data into R (readr)

-   Clean and reshape data before analysis (tidyr and dplyr)

-   Pertform data analysis (dplyr)

-   Create data visualizations(ggplot2)

To install any external packages, we use the function
install.packages().

Here's a little tricky part: You only need to install a library once,
the first time you set up your computer. Below, we will install
Tidyverse with the funcrtion install.packages('tidyverse'). You can type
it directly in the console.

```{r}
#install.packages("tidyverse")
```

Here's what you need to remember: you need to load libraries into the
program's memory each time you use them. That's pretty standard for
programming languages in general; the same applies in Python, for
example.

```{r}
library(tidyverse)
```

The tricky part: - install.packages - once - load the library - each
time you need it

We'll explore Life Expectancy Data by country, 1800 to present with
projections to 2100

Source is [gapminder.org](https://www.gapminder.org/data/)

[Documentation is
here](https://www.gapminder.org/data/documentation/gd004/)

#### Import Life Expectancy Data

For your convenience, I have downloaded a slice of the data into a comma
separated values file, "lex.csv"

```{r}
life_expect <- read.csv("./assets/lex.csv") 
```

Examine Data

```{r}
head(glimpse(life_expect))

```

File types:

-   <chr> = character, text data
-   <dbl> = numeric data

302 columns - Each year life expectancy data 196 rows - each row is a
country (195 counties)

## Dpylr verbs

1.dpylr verbs (actions that allow users to solve data manipulation
challenges), and the questions each enables us to ask

These are the power verbs in R to identify, filter, summarise and create
new fields

Select Arrange Filter Group_by Summarize Mutate

We will deploy these tools in the problems below

Highest life expectancy Which country has the highest life expectancy in
2025?

```{r}

life_expect |> 
  select(country, X2025) |> 
  filter(X2025 == max(X2025, na.rm = TRUE))
  
```

Japan and Singapore, both at 86 years!

**Pipe command**

You noticed an odd set of characters in the previous code: \|\> is a
"pipe" command, which links the code lines in logical order. (Your
program may display it as %\>% - it's the same thing. You can change it
to \|\> in R Studio preferences: Tools \| Global Options \| Code \| Use
Native Pipe Operator).

Think of the pipe command as telling R, "now add this command." So in
the previous code, we started with the data frame life_expect and told R
to add another command, select two columns, country and the year 2025.
And then add another command, filter 2025 by the maximum value.

Using filters

Filters can help us find needles in data haystacks. Let's use a filter
to determine counties with the lowest life expectancy in 2025.

```{r}

life_expect |> 
  select(country, X2025) |> 
  filter(X2025 == min(X2025, na.rm = TRUE))
```

Obviously, we have a data problem, with Hong Kong and Liechtenstein
registering at zero. Let's adjust our query for results greater than
zero

```{r}

life_expect |> 
  select(country, X2025) |> 
  # First filter out zeros and NA values
  filter(X2025 > 0, !is.na(X2025)) |>
  # Then find the minimum among remaining values
  filter(X2025 == min(X2025, na.rm = TRUE)) 
```

What's the story here? Lesotho residents tend to live about 53 years,
according to 2025 data from Gapminder.org, which is 61% as long as the
highest life expectancy in Japan and Singapore of 86 years.

Figure the percentage:

```{r}
52.8/86
```

Using means

Average life expectancy in 2025 The result, 73.1551, is stored in the
variable "life2025"

```{r}
life2025 <- life_expect |> 
  summarize(mean(X2025)) |> 
    pull() # this extracts the result as single numeric value

life2025

```

Get all the data in one place

```{r}
life_expect |> 
  summarize(mean_life = mean(X2025),
            median_life = median(X2025),
            min_life = min(X2025),
            max_life = max(X2025)
)
```

#Average life expectancy in 1925

```{r}
#Here we added na.rm = TRUE to allow the calculation to skip blank values
life1925 <- life_expect |> 
  summarise(mean_life = mean(X1925, na.rm = TRUE)) |> 
    pull() 

life1925

```

**38 years was the average life expectancy in 1925!** Wow.

Percentage change in life expectancy, 1925 v 2025 The numbers are stored
in these variables, so just do the math

```{r}
(life2025-life1925)/life1925 *100
```

Average life expectancy has increased by about 92% since 1925.

#### YOUR TURN

Using the examples above, determine the life expectancy percentage
change from 1975 to 2020.

```{r}

#this portion is blank for the student. 
# The answer is 14.88097
# life1975 <- life_expect |>
#     summarise(mean(X1975, na.rm = TRUE)) |> 
#    pull()
# 
# 
# life2020<- life_expect |>
#    summarise(mean(X2020, na.rm = TRUE)) |>
#     pull()
# #
# #
# (life2020-life1975)/life1975 *100
```

## Grouping and Summing.

2.Grouping and summarizing (counts, sums and averages); Joins (combining
rows from datasets) and what it enables

Counting We know the average life expectancy for 2025, 1975 and 1925.
Let's count the number of countries above these averages

```{r}

life_expect |> 
  select(country, X2025) |> 
  filter(X2025 > life2025) |> 
  count()

```

#### YOUR TURN

Using the example above, determine the the number of countries above the
1975 average life expectancy

```{r}
#this portion is blank for the student. 
# The answer is 113
# life_expect |>
#   select(country, X1975) |>
#   filter(X1975 > life1975) |>
#   count()

```

#### YOUR TURN

Using the example above, determine the the number of countries above the
1925 average life expectancy

```{r}
#this portion is blank for the student. 
# The answer is 64
# life_expect |> 
#   select(country, X1925) |> 
#   filter(X1925 > life1925) |> 
#   count()
```

### Mutate

We are now going to categorize countries as above or below the mean life
expectancy by using the mutate command. Mutate creates a new column and
there are various stratgies to fill in new results. Here we will assign
**above** to all counties in 1925 above the mean life expectancy of
38.11 years. We are using case_when which sets up a logical 'if/else'
statement to apply the category

```{r}
life_1925 <- life_expect |> 
  select(country, X1925) |> 
  mutate(category = case_when(
         X1925 > life1925 ~ "above",
          X1925 < life1925 ~ "below")
)
```

Let's count the results

```{r}
life_1925 |> 
  count(category)

```

#### YOUR TURN

Using the example above, categorize countries as being above or below
the 1975 average life expectancy

```{r}
#this portion is blank for the student. 
# life_1975 <- life_expect |>
#   select(country, X1975) |>
#   mutate(category = case_when(
#          X1975 > life1975 ~ "above",
#           X1975 < life1975 ~ "below")
# )
```

#### YOUR TURN

Using the example above, categorize countries as being above or below
the 2025 average life expectancy

```{r}
#this portion is blank for the student. 
# life_2025 <- life_expect |>
#   select(country, X2025) |>
#   mutate(category = case_when(
#          X2025 > life2025 ~ "above",
#           X2025 < life2025 ~ "below")
# )
```

Now we combine the three results --we rename a column to the same name
for all three time periods so they can be combined. This is a good
example of using mutate.

```{r}
# Transform the dataframes, provide year column, rename value
life_1925 <- life_1925 |> 
  rename(value = X1925) |> 
  mutate(year = 1925)

life_1975 <- life_1975|>
  rename(value = X1975)|>
  mutate(year = 1975)

life_2025 <- life_2025|>
  rename(value = X2025)|>
  mutate(year = 2025)
```

### Joins

Joins (combining rows from datasets) and what it enables

We combine the three using rbind

```{r}

# Combine the dataframes
total <- rbind(life_1925, life_1975, life_2025)

```

Count totals by time period

```{r}
total |> 
  group_by(year, category) |> 
  count(category)

```

#LESSON 4 STARTS HERE

## Basic statistics

### Percentages

Let's determine the percent above, below the mean

Step 1: Count the categories by year

by grouping by year, we can count up the above and below average
countries. The results are in the colummn n

```{r}
total |> 
   group_by(year) |> 
  count(category) 
```

Step Two: Percentage of Whole

We add two lines of code to calculate the percentage. Note how mutate
creates a new column that stores the value. And we format it at the end
to make it look pretty.

The percent calculation divides the category total into the full number
denominator. Because 1925 has 10 null values, we filter those out.
Remember, when dealing with historical data, nations and language can
change over time.

```{r}
total |> 
   group_by(year) |> 
  count(category) |> 
  filter(!is.na(category)) |> #drops the counties without data
  mutate(percent = n/sum(n)) |> 
  mutate(percent = round(percent*100, 1))

```

Percentage change This is a great tool for understanding trends,
especially when dealing with disparate values. The calculation is (new
value - old value)/old value \*100 We'll calculate the percentage change
for Afghanistan, 1925-2025

```{r}

life_expect |> 
  select(country, X1925, X2025) |> 
  filter(country =="Afghanistan") |> 
  mutate(life_change = ((X2025-X1925) / X1925)*100) 
```

So life expectancy improved 67% in Afghanistan from 1925-2025

To improve on the display, we use the package formattable that adds the
percentage sign

```{r}
#install.packages(formattable)
library(formattable)
life_expect |> 
  select(country, X1925, X2025) |> 
  filter(country =="Afghanistan") |> 
  mutate(life_change = ((X2025-X1925) / X1925)) |> 
  mutate(life_change = formattable::percent(life_change, 1))
```

Biggest / smallest improvements in life expectancy Now we will calculate
percentage change life expectancy for all countries And we use slice_max
to extract the top 10 results

```{r}
top <- life_expect |> 
  select(country, X2025, X1925) |> 
  # filter(country =="Afghanistan") |> 
  mutate(pct_change = round((X2025-X1925) / X1925*100,1)) |> 
  slice_max(pct_change, n= 10) 
top
```

And slice_min to extract the bottom 10 results

```{r}
bottom <- life_expect |> 
  select(country, X2025, X1925) |> 
  mutate(pct_change = round(((X2025-X1925) / X1925)*100,1)) |> 
  filter(!is.na(pct_change)) |> #filter out NA values
  filter(X2025 > 1) |> # filter out a random 0 value for Hong Kong in 2025
  slice_min(pct_change, n= 10)
bottom
```

#### ASSIGNMENT

Examine life expectancy for 1800, 1900, 2000. Determine the mean values
for each of those three centuries. Using mutate, create a new category
column for each decade assigning the counties that are above and below
the median value. Compile the three decade into a single dataframe
Produce a single dataframe that counts the number above and below the
median, and calculate a percentage of the whole for only those countries
with reported data

Part 1: Calculate median values by time period

```{r}
#this portion is blank for the student. 
# life1800 <- life_expect |>
#    summarise(mean(X1800, na.rm = TRUE)) |> 
#   pull() # this extracts the result as single numeric value
# 
# life1900 <- life_expect |>
#    summarise(mean(X1900, na.rm = TRUE)) |> 
#   pull()
# 
# life2000 <- life_expect |>
#    summarise(mean(X2000, na.rm = TRUE)) |> 
#   pull()


```

Part 2: Categorize by time period above / below median

```{r}
#this portion is blank for the student. 
# 
# life_1800 <- life_expect |>
#   select(country, X1800) |>
#   mutate(category = case_when(
#          X1800 > life1800 ~ "above",
#           X1800 < life1800 ~ "below")
# )
# 
# life_1900 <- life_expect |>
#   select(country, X1900) |>
#   mutate(category = case_when(
#          X1900 > life1900 ~ "above",
#           X1900 < life1900 ~ "below")
# )
# 
# life_2000 <- life_expect |>
#   select(country, X2000) |>
#   mutate(category = case_when(
#          X2000 > life2000 ~ "above",
#           X2000 < life2000 ~ "below")
# )

```

Part 3: Compile

```{r}
#this portion is blank for the student. 
# life_1800 <- life_1800|>
#   rename(value = X1800)|>
#   mutate(year = 1800)
# 
# life_1900 <- life_1900|>
#   rename(value = X1900)|>
#   mutate(year = 1900)
# 
# life_2000 <- life_2000|>
#   rename(value = X2000)|>
#   mutate(year = 2000)
# 
# total1800_2000 <- rbind(life_1800, life_1900, life_2000)

```

Part 4: Calculate totals, percentages

```{r}
#this portion is blank for the student. 
 # total1800_2000 |> 
 #   group_by(year) |> 
 #  count(category) |> 
 #  filter(!is.na(category)) |> #drops the counties without data
 #  mutate(percent = n/sum(n)) |> 
 #  mutate(percent = round(percent*100, 1))


# Answer:
# year. category n %
# 1800	above	95	51.1	
# 1800	below	91	48.9	
# 1900	above	81	43.5	
# 1900	below	105	56.5	
# 2000	above	115	58.7	
# 2000	below	81	41.3	
```

Calculate the percentage change from 1800 to 1900 and from 1900 to 2000
Create two tables with the top 10 biggest increases for each period

```{r}
#this portion is blank for the student. 
# change <- life_expect |>
#   select(country, X1800, X1900, X2000) |>
#   mutate(pct_1900_1800 = ((X1900-X1800) / X1800)*100) |>
#   mutate(pct_2000_1900 = ((X2000-X1900) / X1900)*100)
# 
# pct_1900_1800 <- change |>
#   select(country, X1900, X1800, pct_1900_1800) |>
#   slice_max(pct_1900_1800, n=10)
# 
# pct_2000_1900 <- change |>
#   select(country, X2000, X1900, pct_2000_1900) |>
#   slice_max(pct_2000_1900, n=10)

```

Distribution of the data How are countries grouped? Were there many
below the average?

```{r}
life_1925 |> 
   summary(value)
```

We see the lowest value is 21 years life expectancy, the first quartile
is up to 32 years, the median value was 36 years, the mean or average
was 38 years, the third quartile was 40 years and the maximum value was
64 years.

Let's explore the distribution by using a tool called standard deviation

```{r}
life_1925_sd <- life_1925 |> 
  summarize(standard_deviation = sd(value, na.rm = TRUE))
life_1925_sd
```

So we know the mean life expectancy is 38 years. Tells us that 68% of
the countries in 1925 fall within one standard deviation, or 9.3 years,
of the mean. In other words, 68% of the countries had a life expectancy
between 28.3 years and 46.3 years.

Just about all countries, or 95%, fall within two standard deviations
from the mean, or 18.6 years; that would be between 19.4 years and 56.6
years of age.

And nearly all counties are within three standard deviations, or 27.9
years, or 10.1 years and 65.9 years.

This means that any value within 9.3 percentage points of the mean (38
years), either above or below, is within the first group. This provides
a sense of outliers and how dispersed the data may be in the dataset.

```{r}
life_quartiles <- life_1925 |> 
  filter(!is.na(value)) |> 
  mutate(quartile = cut(value, 
                         breaks = quantile(value, probs = seq(0, 1, 0.25), na.rm = TRUE),#quantile allows division into any grouping; this gives the min value, 0; the max value, 1; and the step value, 0.25
                         labels = c("Q1", "Q2", "Q3", "Q4"),
                         include.lowest = TRUE)) #keeps lowest value in first group


quartile_stats <- life_quartiles |> 
  group_by(quartile) |> 
  summarize(
    count = n(),
    mean_value = mean(value, na.rm = TRUE),
    sd_value = sd(value, na.rm = TRUE),
    min_value = min(value, na.rm = TRUE),
    max_value = max(value, na.rm = TRUE)
  )
```

Another way to examine data is by calculating results in percentiles. We
use the command ntile

```{r}
life_1925 |> 
  mutate(percentile = ntile(value, 100)) 
```

List all countries in the top 90th percentile for life expectancy in
1925

```{r}
life_1925 |> 
  mutate(percentile = ntile(value, 100)) |> 
  filter(percentile > 89) |> 
  arrange(desc(percentile))
```

#### YOUR TURN

Using the examples above

```{r}



```

## Data Visualization

R has a variety of excellent data visualization tools. You can build
maps, interactive graphics and many different types of static charts.
You can also directly output to external tools such as Datawrapper,
which you will see in a subsequent lesson.

To start, you're going to work with the "top" dataframe that we built
earlier, that shows the top 10 countries by percentage change in life
expectancy from 1925 to 2025.

```{r}
ggplot(data=top) +
  geom_col(mapping=aes(x=pct_change, y=country)) 

```

**That's ugly. Add some color: fill=n**

```{r}
ggplot(data=top) +
  geom_col(mapping=aes(x=pct_change, y=country, fill=pct_change)) 

```

**Where's the "pipe"?** In ggplot2, the command lines are linked by a +
sign and not the pipe operator \|\> It's a quirk in R and we hope they
fix it at some point. Don't hold your breath, however. Until then, use
the + operator to link lines in ggplot2

**Ditch the legend: theme(legend.position = "none")**

```{r}

ggplot(top,aes(x = pct_change, y = country,
             fill = pct_change)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none")

```

**Add Headlines, Annotations and Credits: labs(title =**

```{r}

ggplot(top,aes(x = pct_change, y = country,
             fill = pct_change)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
#This is your title sequence
  labs(title = "Countries with biggest life expectancy gains, 1925-2025",
       subtitle = "Top 10 countries by percentage change",
       caption = "Source: Gapminder - https://www.gapminder.org/data/ Graphic by Rob Wells, 5-27-2025",
       y="Country",
       x="Pct Change")
```

**Sorting a chart** The bars need to be sorted, highest to lowest. Use
reorder()

```{r}
ggplot(top, aes(x = pct_change, y = reorder(country, pct_change),
                fill = pct_change)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
#This is your title sequence
  labs(title = "Countries with biggest life expectancy gains, 1925-2025",
       subtitle = "Top 10 countries by percentage change",
       caption = "Source: Gapminder - https://www.gapminder.org/data/ Graphic by Rob Wells, 5-27-2025",
       y="Country",
       x="Pct Change")
```

## Text Analysis

R can also help you find gems in big stacks of documents. We will work
with a lengthy document to learn common phrases. In this case, it's a
62-page transcript of the year-end news conference by Russian President
Vladimir Putin, held of Dec. 26, 2024.

To do this work, you will install tidytext, a powerful program that
prepares text for machine learning and natural language processing.

```{r}
install.packages("tidytext")
library(tidytext)
```

To speed thing up, I downloaded the transcript from the BBC and
processed it into a raw text file, called putin.txt

**Tokenize**

```{r}
putin <- read_lines("./assets/putin.txt") #This reads the text file into R.

putin_df <- tibble(putin,) #This converts the file into a dataframe, one column, 3,624 rows. Man, that guy can talk!
```

**Tokenize data** This process takes a sentence and makes it one row per
word. Using the previous sentence, tokenization will do the following: 1
This 2 process 3 takes 4 a 5 sentence 6 and 7 makes 8 it 9 one 10 row 11
per 12 word.

```{r}
putin_tokenized <- putin_df %>%
  unnest_tokens(word,putin)

head(putin_tokenized, 20)
```

So tokenizing this 62-page transcript yields a dataframe with 37,007
rows, one line per word. Putting the transcript into such a structured
data format allows R to perform many computations on the text such as
counting the frequency of word pairs, or bigrams.

**Word Count**

```{r}
putin_word_ct <- putin_tokenized %>%
  count(word, sort=TRUE)

head(putin_word_ct, 20)
```

**Remove stopwords** We can see from the results above that the program
correctly counted "the" as the most frequently used word. Great, but
that doesn't give us much insight into what Putin said.

We can cut the articles, pronouns and other words to get to the meat of
the text. The tidytext package includes the stop_words dataset. It
contains, as of this writing, 1,149 words that data scientists and
linguistic nerds felt could be removed from sentences because they don't
add meaning. Filtering out these words can help focus on the more
meaningful content, making it easier to uncover trends, themes, and key
information in large amounts of text. Obviously, we have different
priorities and we may or may not want to use stop_words or we have want
to provide a customized list of stop words.

The stop_words list is derived from three separate lists, or lexicons:
SMART (571 words), onix (404 words), and snowball (174 words)

The ONIX lexicon comes from the Open Information Exchange and is often
used in text mining and natural language processing.

The Snowball lexicon is part of a broader project that has algorithms
that simplify words in different languages by reducing them to their
root form. It's best known for the Porter stemming algorithm, which, for
example, changes "running" to "run."

Lastly, the SMART lexicon is a set of common words, like "and," "the,"
and "is," and it comes from the SMART Information Retrieval System,
created at Cornell University in the 1960s.

```{r}
data(stop_words)

test <- stop_words %>% 
  as.data.frame()

head(test)
```

**Remove stopwords**

```{r}

data(stop_words)

putin_tokenized_clean <- putin_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

putin_word_ct_clean <- putin_tokenized_clean %>%
  count(word, sort=TRUE)

```

###YOUR TURN Compare the putin_word_ct_clean to the putin_word_ct. Write
three sentences comparing the two and provide examples

**Bigrams**

Now we will count the most common two-word phrases, or bigrams, in the
Putin press conference transcript.

```{r}
bigrams <- putin_df  |> 
  unnest_tokens(bigram, putin, token="ngrams", n=2) |> 
    separate(bigram, c("word1", "word2"), sep = " ")

#Filter out stop words.
bigrams_cleaned <- bigrams |> 
  anti_join(stop_words, by = c("word1" = "word"))  |> #filters out words based on the stop_word list
  anti_join(stop_words, by = c("word2" = "word")) |> 
  filter(!is.na(word1)) |> #eliminates NA values
  count(word1, word2, sort = TRUE)
```

A little formatting for later

```{r}
bigrams_cleaned <- bigrams_cleaned |> 
  mutate(bigram = (paste0(word1, " ", word2))) |> #rejoins the phrases into a single column
  select(bigram, n, -word1, -word2) #eliminates the separate columns, reorders

head(bigrams_cleaned, 20)
```

We can see from the results that Vladimir Putin is the most common
phrase in the transcript, which isn't surprising. But look through the
results and you can see the War Against Ukraine is the most prominent
topic: "special military" and "military operation" are in the top 10
results.

<br>

### YOUR TURN

Construct a ggplot chart with the top 25 phrases from the Putin press
conference. Display the results in descending order using reorder()

```{r}
#this portion is blank for the student. 
#putin_25 <- head(bigrams_cleaned,25)

# ggplot(putin_25,aes(x = n, y = reorder(bigram, n), 
#              fill = n)) +
#   geom_col(position = "dodge") + 
#   theme(legend.position = "none") +
# #This is your title sequence
#   labs(title = "Top phrases from Putin's 2024 press conference",
#        subtitle = "Bigrams from 62-page transcript",
#        caption = "Source: BBC Graphic by Rob Wells, 5-27-2025",
#        y="Bigram",
#        x="Count")

```

**2) Chart Calls by Day** Use the code above and add a ggplot chart to
it

```{r}

```

<br>

|        |
|:-------|
| #Notes |

<https://thescoop.org/data_journalism_book/textanalysis.html>

Now we're going to clean the data using the janitor package in R

```{r}
life_expect <- read_csv("./assets/lex.csv") |> 
  clean_names()
```

Congratulations, you just received your first error message in R. What
happened? The code above tried to execute a command, clean_names, when
the library wasn't loaded in R. You need to install janitor and load the
janitor package. Go ahead and do this and then rerun the code above.

```{r}
#this portion is blank for the student. 
#install.packages(janitor)
#library(janitor)
# life_expect <- read_csv("./assets/lex.csv") |> 
#   clean_names()

```

Get a list of countries to group them by continent, asia, europe etc Get
list of countries with median income

Here's another way to combine the three results This code creates a new
table that puts all the the calculations into one place for easier
analysis

```{r}

above_average <- data.frame(
  Year = c("1925", "1975", "2025"),
  Countries_Above_Average = c(
    sum(life_expect$X1925 > life1925, na.rm = TRUE),
    sum(life_expect$X1975 > life1975, na.rm = TRUE),
    sum(life_expect$X2025 > life2025, na.rm = TRUE)
  )
)

above_average
```

### Percentages

Ok, all of that is fine, but do we have a consistent denominator, the
same number of countries over time? Remember, when dealing with
historical data, nations and language can change over time.

Count total number of countries

```{r}
life_expect |> 
  select(country, X2025, X1975, X1925) |> 
  summarize(
    count_2025 = sum(!is.na(X2025)),
    count_1975 = sum(!is.na(X1975)),
    count_1925 = sum(!is.na(X1925))
  )

```

```{r}


above_average <- data.frame(
  Year = c("1925", "1975", "2025"),
  Countries_Above_Average = c(
    sum(life_expect$X1925 > life1925, na.rm = TRUE),
    sum(life_expect$X1975 > life1975, na.rm = TRUE),
    sum(life_expect$X2025 > life2025, na.rm = TRUE)
  ),
  Total_Countries_With_Data = c(
    sum(!is.na(life_expect$X1925)),
    sum(!is.na(life_expect$X1975)),
    sum(!is.na(life_expect$X2025))
  )
) |> 
  mutate(Percent_Above_Average = round(
  (Countries_Above_Average / Total_Countries_With_Data) * 100, 
  1
))

above_average



```

We see from our quick glance that Andorra lacks entries for the first
few years. Let's focus on Andorra using a filter

```{r}

Andorra <- life_expect |> 
  filter(country == "Andorra") |> 
   # Convert to long format so we can work with years as values
  pivot_longer(
    cols = starts_with("X"), 
    names_to = "year",
    values_to = "life_expectancy"
  ) |>
  # Remove the "X" prefix from year names and convert to numeric
  mutate(year = as.numeric(gsub("X", "", year))) |>
  # Keep only years with non-NA values
  filter(!is.na(life_expectancy)) |>
  # Sort by year (optional)
  arrange(year)


```
